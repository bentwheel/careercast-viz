---
title: "What Actuaries Do Is Changing"
output: md_document
---

## Be an Actuary: Low stress, high pay. Case closed?

At the end of 2013, I decided prior to beginning my junior year at the University of Pennsylvania that I'd wanted to be an actuary. One of the many compelling forces that had brought me to this particular career strategy was the widely cited career outlook for actuaries around that period of time. For example, from [BeAnActuary.org](https://beanactuary.org/why/?fa=a-top-ranked-job):

![](https://github.com/bentwheel/careercast-viz/blob/master/topjob.JPG?raw=true)

The link whisks you off to a list of articles, not a single one newer than 2016, promoting the high-pay/low-stress benefits of becoming an actuary. It sure is a little stale, and so much has changed with respect to the bucket of functions we use to map data to decisions since 2016. So I took to the web to see how the "Actuary" career was faring in 2019 on the same career ranking site that was boasting top billing for the profession back in 2013.

And, well, things have changed.

## Web Scraping: Now (mostly) legal, still the same amount of fun as always

The web is rich with info, and I love to scrape it. And thanks to a [relatively recent 9th Circuit case](https://www.vice.com/en/article/9kek83/linkedin-data-scraping-lawsuit-shot-down), provided there's no specific legislation barring the practice or conflicting circuit court of appeals ruling that requires SCOTUS intervention, it's now legal (at least, for non-commercial reasons) and is likely to remain that way.

Scraping the web is easy to do, and is a great way to make an un-automated task sail by pretty quickly. If you're a Python person, you can use the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) package (`pip install beautifulsoup4`). If you're an R person, then [rvest](https://github.com/tidyverse/rvest) is the package for you. Since I like to generally scrape data to `ggplot` it, I almost always use the `tidyverse`'s `rvest` package (as in "harvest"), but you should live your best life.

Also, it's helpful to have a [CSS SelectorGadget](https://selectorgadget.com/) or similar browser extension to give your webscraping efforts more surgical precision, but sometimes it's easier to look at the page source, or grab the HTML broadly and whittle it down in code.

## CareerCast.com: Does the Actuary gig still get top billing?

I turned my attention to the [CareerCast Top 200 Careers of 2013](https://www.careercast.com/jobs-rated/best-worst-jobs-2013) report, which ranks careers on an overall index based on job stress levels, projected career growth outlook, and work environment. Sure enough, in 2013, there it was in the number one slot:

![](https://github.com/bentwheel/careercast-viz/blob/master/topjob2013.JPG?raw=true)

I love being an actuary, and I think most actuaries love being actuaries, so I shimmied on over to the [CareerCast Top 200 Careers of 2019](https://www.careercast.com/jobs-rated/2019-jobs-rated-report) report, and there it was, halfway down the Top 20 at #10.

![](https://github.com/bentwheel/careercast-viz/blob/master/actuaryjob2019.JPG?raw=true)

What happened? And what had taken the top spot? Was it... no.. it couldn't be. Could it?

![](https://github.com/bentwheel/careercast-viz/blob/master/dsjob2019.JPG?raw=true)

Of course it's No. 1. *Touch√©*.

![](https://github.com/bentwheel/careercast-viz/blob/master/newman.jpg?raw=true)


```{r get_data_2019, include=FALSE}

library(tidyverse)
library(rvest)
library(xml2)
library(kableExtra)

scrape_careercast_page <- function(url) 
{
  # Scrape HTML file from 2019 job report
  
  rankings_html <- read_html(url)
  
  get_rankings_data <- as_list(xml2::xml_find_all(rankings_html,'//*[contains(concat( " ", @class, " " ), concat( " ", "aiNormalizedNumber", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "views-field-field-name", " " ))]')) %>% 
    flatten() %>% 
    flatten() %>% 
    flatten() %>% 
    tibble(data = str_trim(.)) %>% 
    select(data) %>% 
    filter(data != "") %>% 
    mutate(n = (row_number() - 1) %% 6,
           label = case_when(n == 0 ~ "rank",
                             n == 1 ~ "career",
                             n == 2 ~ "overall_rank",
                             n == 3 ~ "work_environment_rank",
                             n == 4 ~ "stress_rank",
                             n == 5 ~ "proj_growth_rank")) %>% 
    filter(n != 0) %>% 
    arrange(n) %>% 
    mutate(n = (row_number() - 1) %% 20) %>% 
    group_by(n) %>% 
    pivot_wider(names_from = label, values_from = data) %>% 
    ungroup() %>% 
    select(-n)

    return(get_rankings_data)  
}

```

```{r display_2019_data}

rankings_url <- "https://www.careercast.com/jobs-rated/2019-jobs-rated-report?page=0"
scrape_careercast_page(rankings_url) %>% kable()
  
```

As you can see, I went ahead and made a handy little function to handle all the webscraping. All I need to do is insert the URL for the year I want to scrape, and boom! Data.

There's a reason I made this a function - it's so I could scrape the CareerCast site from the last ten or so years, and see what careers have risen and fallen in their ranking methodology as well as which have remained completely consistent and stable over the last decade. 

My only hope is that their web design philosophy and CSS naming conventions have remained just as consistent!

By the way, since it's only ten years, I've chosen to do it by hand, but if we were doing any more than a decade, this is the kind of stuff `purrr::map()` is made for. Once we've got the data pulled, we're ready to plot.

```{r pull_decade, include=FALSE}

# 2019
rankings_url <- "https://www.careercast.com/jobs-rated/2019-jobs-rated-report?page=0"
data <- scrape_careercast_page(rankings_url) %>% 
  mutate(year = 2019)

# 2018 - there is an issue here that has the overall ranking starting at 2 instead of 1. I've corrected for this.
rankings_url <- "https://www.careercast.com/jobs-rated/2018-jobs-rated-report"
data2018 <- scrape_careercast_page(rankings_url) %>% 
  mutate(year = 2018)
  
data2018 <- data2018 %>% 
  separate(overall_rank,  sep="/", into=c("num","den")) %>% 
  mutate(num = as.character(as.numeric(num) - 1)) %>% 
  unite(col="overall_rank", num, den, sep="/")

data <- data %>% 
  union_all(data2018)

# 2017 - no issues
rankings_url <- "https://www.careercast.com/jobs-rated/2017-jobs-rated-report"
data2017 <- scrape_careercast_page(rankings_url) %>% 
  mutate(year = 2017)

data <- data %>% 
  union_all(data2017)

# 2016 - URL format is slightly different.
rankings_url <- "https://www.careercast.com/jobs-rated/jobs-rated-report-2016-ranking-200-jobs"
data2016 <- scrape_careercast_page(rankings_url) %>% 
  mutate(year = 2016)

data <- data %>% 
  union_all(data2016)

# 2015 - again slightly different URL
rankings_url <- "https://www.careercast.com/jobs-rated/jobs-rated-report-2015-ranking-top-200-jobs"
data2015 <- scrape_careercast_page(rankings_url) %>% 
  mutate(year = 2015)

data <- data %>% 
  union_all(data2015)

# 2014 - again a slightly different URL
rankings_url <- "https://www.careercast.com/jobs-rated/jobs-rated-2014-ranking-200-jobs-best-worst"
data2014 <- scrape_careercast_page(rankings_url) %>% 
  mutate(year = 2014)

data <- data %>% 
  union_all(data2014)

# 2013 - again a slightly different URL than last year
rankings_url <- "https://www.careercast.com/jobs-rated/best-worst-jobs-2013"
data2013 <- scrape_careercast_page(rankings_url) %>% 
  mutate(year = 2013)

data <- data %>% 
  union_all(data2013)

# 2012 - again a slightly different URL than last year
rankings_url <- "https://www.careercast.com/jobs-rated/2012-ranking-200-jobs-best-worst"
data2012 <- scrape_careercast_page(rankings_url) %>% 
  mutate(year = 2012)

data <- data %>% 
  union_all(data2012)

# 2011 - the same URL format as the 2012 report! huzzah!
rankings_url <- "https://www.careercast.com/jobs-rated/2011-ranking-200-jobs-best-worst"
data2011 <- scrape_careercast_page(rankings_url) %>% 
  mutate(year = 2011)

data <- data %>% 
  union_all(data2011)

# 2010 is a 404 error, so we'll stop there.
```


```{r plotting, include=FALSE}

# As usual, we'll set up a plot function that gives a plot of the ranking we want - overall,
# job growth, stress, etc.


plotRanking <- function(metric, data)
{
  
  metricLabel = ifelse(metric == "overall_rank","Overall Rank",
                 ifelse(metric == "work_environment_rank","Work Environment Rank",
                    ifelse(metric == "stress_rank","Stress Rank", "Projected Growth Rank")))
  
  print(metricLabel)
  
  plot.data <- data %>% 
    select(career, eval(metric), year) %>% 
    separate(eval(metric), into=c("num","denom"), sep="/") %>% 
    mutate(num = as.numeric(num))
  
  plot.data <- plot.data %>% 
    group_by(career) %>% 
    summarize(n = n()) %>% 
    filter(n >= 6) %>% 
    left_join(plot.data)
  
  plot <- plot.data %>% 
    ggplot(aes(x = year, y = num, color = career)) + 
    geom_line(size = 1.5) +
    geom_point(size = 2) + 
    scale_y_reverse(labels = scales::label_ordinal(), breaks=seq(1:20)) +
    labs(x = "Year",
         y = "Rank",
         title = paste0(metricLabel, " over the Last Decade-ish"),
         subtitle = "CareerCast.com Career Rankings, 2011 - 2019") +
    theme_minimal()
    
  plot
}
```

```{r plots}
plotRanking("overall_rank", data=data)

```

